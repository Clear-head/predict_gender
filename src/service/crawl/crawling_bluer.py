"""
Bluer ì›¹ì‚¬ì´íŠ¸ ìŒì‹ì  í¬ë¡¤ë§ ëª¨ë“ˆ (ë©”ëª¨ë¦¬ ìµœì í™” + ë´‡ ìš°íšŒ + ë³‘ë ¬ ì²˜ë¦¬)
1ë‹¨ê³„: ì „ì²´ ëª©ë¡ ìˆ˜ì§‘ â†’ 2ë‹¨ê³„: ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§
"""
import asyncio
from playwright.async_api import async_playwright, TimeoutError, Page
import sys, os
from dotenv import load_dotenv

load_dotenv(dotenv_path="src/.env")

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.logger.custom_logger import get_logger

# ê³µí†µ ëª¨ë“ˆ import
from src.service.crawl.utils.optimized_browser_manager import OptimizedBrowserManager
from src.service.crawl.utils.human_like_actions import HumanLikeActions
from src.service.crawl.utils.scroll_helper import PageNavigator
from src.service.crawl.utils.store_detail_extractor import StoreDetailExtractor
from src.service.crawl.utils.store_data_saver import StoreDataSaver
from src.service.crawl.utils.search_strategy import NaverMapSearchStrategy
from src.service.crawl.utils.crawling_manager import CrawlingManager


class BluerRestaurantCrawler:
    """Bluer ì›¹ì‚¬ì´íŠ¸ ìŒì‹ì  í¬ë¡¤ë§ í´ë˜ìŠ¤ (ë³‘ë ¬ ì²˜ë¦¬)"""
    
    RESTART_INTERVAL = 50  # 50ê°œë§ˆë‹¤ ì»¨í…ìŠ¤íŠ¸ ì¬ì‹œì‘
    
    def __init__(self, headless: bool = False):
        self.logger = get_logger(__name__)
        self.headless = headless
        self.bluer_url = "https://www.bluer.co.kr/search?query=&foodType=&foodTypeDetail=&feature=112&location=&locationDetail=&area=&areaDetail=&ribbonType=&priceRangeMin=0&priceRangeMax=1000&week=&hourMin=0&hourMax=48&year=&evaluate=&sort=&listType=card&isSearchName=false&isBrand=false&isAround=false&isMap=false&zone1=&zone2=&food1=&food2=&zone2Lat=&zone2Lng=&distance=1000&isMapList=false#restaurant-filter-bottom"
        self.data_saver = StoreDataSaver()
        self.search_strategy = NaverMapSearchStrategy()
        self.human_actions = HumanLikeActions()
        self.success_count = 0
        self.fail_count = 0
    
    async def crawl_all_pages(self, delay: int = 5, naver_delay: int = 20):
        """
        Bluer ì „ì²´ í˜ì´ì§€ ë³‘ë ¬ í¬ë¡¤ë§
        1ë‹¨ê³„: Bluerì—ì„œ ì „ì²´ ëª©ë¡ ìˆ˜ì§‘ â†’ 2ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§
        
        Args:
            delay: Bluer í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)
            naver_delay: ë„¤ì´ë²„ ì§€ë„ í¬ë¡¤ë§ ë”œë ˆì´ (ì´ˆ)
        """
        async with async_playwright() as p:
            # 1ë‹¨ê³„: Bluerì—ì„œ ì „ì²´ ìŒì‹ì  ëª©ë¡ ìˆ˜ì§‘
            self.logger.info("1ë‹¨ê³„: Bluer ì „ì²´ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘")
            
            all_restaurants = await self._collect_all_restaurants(p, delay)
            
            if not all_restaurants:
                self.logger.warning("ìˆ˜ì§‘ëœ ìŒì‹ì ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            total = len(all_restaurants)
            self.logger.info(f"ì´ {total}ê°œ ìŒì‹ì  ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 2ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ì—ì„œ ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§
            self.logger.info("2ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘")
            self.logger.info(f"ë°°ì¹˜ í¬ê¸°: {self.RESTART_INTERVAL}ê°œ")
            self.logger.info(f"ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {(total + self.RESTART_INTERVAL - 1) // self.RESTART_INTERVAL}ê°œ")
            
            naver_browser = await OptimizedBrowserManager.create_optimized_browser(p, self.headless)
            
            try:
                for batch_start in range(0, total, self.RESTART_INTERVAL):
                    batch_end = min(batch_start + self.RESTART_INTERVAL, total)
                    batch = all_restaurants[batch_start:batch_end]
                    
                    batch_num = batch_start // self.RESTART_INTERVAL + 1
                    total_batches = (total + self.RESTART_INTERVAL - 1) // self.RESTART_INTERVAL
                    
                    self.logger.info(f"ë°°ì¹˜ {batch_num}/{total_batches}: {batch_start+1}~{batch_end}/{total}")
                    
                    # ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    context = await OptimizedBrowserManager.create_stealth_context(naver_browser)
                    page = await context.new_page()
                    
                    try:
                        await self._process_batch_parallel(
                            page, batch, batch_start, total, naver_delay
                        )
                    except Exception as e:
                        self.logger.error(f"ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        import traceback
                        self.logger.error(traceback.format_exc())
                    finally:
                        await context.close()
                        await asyncio.sleep(3)
                        
                        # ë°°ì¹˜ ê°„ íœ´ì‹
                        if batch_end < total:
                            import random
                            rest_time = random.uniform(20, 40)
                            self.logger.info(f"ë°°ì¹˜ {batch_num} ì™„ë£Œ, {rest_time:.0f}ì´ˆ íœ´ì‹...\n")
                            await asyncio.sleep(rest_time)
                
                # ìµœì¢… ê²°ê³¼
                self.logger.info(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
                self.logger.info(f"ì´ ì²˜ë¦¬: {total}ê°œ")
                self.logger.info(f"ì„±ê³µ: {self.success_count}ê°œ")
                self.logger.info(f"ì‹¤íŒ¨: {self.fail_count}ê°œ")
                if total > 0:
                    self.logger.info(f"   ì„±ê³µë¥ : {self.success_count/total*100:.1f}%")
                
            finally:
                await naver_browser.close()
    
    async def _collect_all_restaurants(self, playwright, delay: int) -> list:
        """Bluerì—ì„œ ì „ì²´ ìŒì‹ì  ëª©ë¡ë§Œ ìˆ˜ì§‘"""
        browser = await playwright.chromium.launch(headless=self.headless)
        page = await browser.new_page()
        
        all_restaurants = []
        
        try:
            self.logger.info(f"Bluer í˜ì´ì§€ ì ‘ì† ì¤‘...")
            await page.goto(self.bluer_url, wait_until='networkidle')
            await asyncio.sleep(3)
            
            current_page = 1
            
            while True:
                self.logger.info(f"Bluer í˜ì´ì§€ {current_page} ìˆ˜ì§‘ ì¤‘...")
                
                restaurants = await self._extract_restaurants_from_page(page)
                
                if restaurants:
                    self.logger.info(f"í˜ì´ì§€ {current_page}: {len(restaurants)}ê°œ ìˆ˜ì§‘ (ëˆ„ì  {len(all_restaurants) + len(restaurants)}ê°œ)")
                    all_restaurants.extend(restaurants)
                else:
                    self.logger.warning(f"í˜ì´ì§€ {current_page}ì—ì„œ ìŒì‹ì ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    break
                
                has_next = await PageNavigator.go_to_next_page_bluer(page)
                
                if not has_next:
                    self.logger.info(f"ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ (ì´ {current_page}í˜ì´ì§€)")
                    break
                
                current_page += 1
                await asyncio.sleep(delay)
            
        except Exception as e:
            self.logger.error(f"Bluer ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        finally:
            await browser.close()
        
        return all_restaurants
    
    async def _extract_restaurants_from_page(self, page: Page) -> list:
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ìŒì‹ì  ì´ë¦„ê³¼ ì£¼ì†Œ ì¶”ì¶œ"""
        restaurants = []
        
        try:
            await page.wait_for_selector('#list-restaurant', timeout=10000)
            await asyncio.sleep(2)
            
            list_items = await page.locator('#list-restaurant > li').all()
            
            for idx, item in enumerate(list_items, 1):
                try:
                    # ìŒì‹ì ëª…
                    name_selector = 'div > header > div.header-title > div:nth-child(2) > h3'
                    name_element = item.locator(name_selector)
                    
                    if await name_element.count() > 0:
                        name = await name_element.inner_text(timeout=3000)
                        name = name.strip()
                    else:
                        continue
                    
                    # ì£¼ì†Œ
                    address_selector = 'div > div > div.info > div:nth-child(1) > div'
                    address_element = item.locator(address_selector)
                    
                    if await address_element.count() > 0:
                        address = await address_element.inner_text(timeout=3000)
                        address = address.strip()
                    else:
                        address = ""
                    
                    if name:
                        restaurants.append((name, address))
                    
                except Exception as item_error:
                    self.logger.error(f"ì•„ì´í…œ {idx} ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {item_error}")
                    continue
            
        except TimeoutError:
            self.logger.error("ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.error(f"ìŒì‹ì  ëª©ë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return restaurants
    
    async def _process_batch_parallel(
        self, 
        page: Page, 
        batch: list, 
        batch_start: int, 
        total: int, 
        delay: int
    ):
        """ë°°ì¹˜ ë³‘ë ¬ í¬ë¡¤ë§"""
        try:
            # ========================================
            # ğŸ”¥ ë³‘ë ¬ ì²˜ë¦¬: CrawlingManager ì‚¬ìš©
            # ========================================
            crawling_manager = CrawlingManager("Bluer")
            
            await crawling_manager.execute_crawling_with_save(
                stores=batch,
                crawl_func=lambda store, idx, t: self._crawl_single_store_parallel(page, store),
                save_func=self._save_wrapper_with_total(batch_start, total),
                delay=delay
            )
            
            # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
            self.success_count += crawling_manager.success_count
            self.fail_count += crawling_manager.fail_count
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
    
    async def _crawl_single_store_parallel(self, page: Page, store: tuple):
        """
        ë‹¨ì¼ ë§¤ì¥ í¬ë¡¤ë§ (ë³‘ë ¬ìš©)
        
        Returns:
            Tuple: (store_data, name) ë˜ëŠ” None
        """
        name, address = store
        
        try:
            # ê²€ìƒ‰ ì „ëµ ì‚¬ìš©
            async def extract_callback(entry_frame, page):
                extractor = StoreDetailExtractor(entry_frame, page)
                return await extractor.extract_all_details()
            
            store_data = await self.search_strategy.search_with_multiple_strategies(
                page=page,
                store_name=name,
                road_address=address,
                extractor_callback=extract_callback
            )
            
            if store_data:
                # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                await OptimizedBrowserManager.clear_page_resources(page)
                return (store_data, name)
            
            return None
            
        except Exception as e:
            self.logger.error(f"'{name}' í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _save_wrapper_with_total(self, batch_start: int, total: int):
        """ì €ì¥ ë˜í¼ íŒ©í† ë¦¬"""
        async def wrapper(idx: int, _, store_data_tuple, store_name: str):
            if store_data_tuple is None:
                return (False, "í¬ë¡¤ë§ ì‹¤íŒ¨")
            
            store_data, actual_name = store_data_tuple
            global_idx = batch_start + idx
            
            return await self.data_saver.save_store_data(
                idx=global_idx,
                total=total,
                store_data=store_data,
                store_name=actual_name,
                log_prefix="Bluer"
            )
        
        return wrapper


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger = get_logger(__name__)
    
    logger.info("Bluer ìŒì‹ì  í¬ë¡¤ëŸ¬ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)")
    
    try:
        crawler = BluerRestaurantCrawler(headless=False)
        
        await crawler.crawl_all_pages(
            delay=5,
            naver_delay=15
        )
        
        logger.info("í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(traceback.format_exc())